"""Base data collators for audio-language tasks."""

from __future__ import annotations

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Dict, List, Sequence

import torch


@dataclass
class BaseAudioTextCollator(ABC):
    """Abstract base class for audio+text data collators.

    All collators using chat template format should inherit from this class.
    This implements the common pattern of:
    1. Building chat-formatted prompts with audio + text instruction
    2. Processing with the model's processor
    3. Creating labels with proper masking (padding, audio tokens, instruction tokens)

    Subclasses must implement:
    - _build_instruction: Create the instruction text for the user message
    - _get_label_text: Extract the target label/text from a feature
    """

    processor: Any
    sampling_rate: int
    include_transcript: bool = True

    def _build_instruction(self, feature: Dict[str, Any]) -> str:
        """Build the instruction text for the user message.

        Args:
            feature: A single example from the dataset

        Returns:
            The instruction text to be included in the user message
        """
        raise NotImplementedError("Subclasses must implement _build_instruction")

    def _get_label_text(self, feature: Dict[str, Any]) -> str:
        """Extract the label/target text from a feature.

        Args:
            feature: A single example from the dataset

        Returns:
            The target text that should be generated by the assistant
        """
        raise NotImplementedError("Subclasses must implement _get_label_text")

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """Prepare batch tensors for the trainer.

        This implements the standard pattern:
        1. Extract audio arrays and target labels
        2. Build chat-formatted prompts
        3. Process with the model processor
        4. Create labels with proper masking
        """
        # Filter out corrupted audio samples (optional - only for tasks that need it)
        valid_features = self._filter_corrupted_audio(features)
        if not valid_features:
            raise RuntimeError("All audio samples in batch are corrupted")

        # Extract audio and labels
        audio_arrays = [feature["audio"]["array"] for feature in valid_features]
        label_texts = [self._get_label_text(feature) for feature in valid_features]

        # Configure tokenizer padding
        tokenizer = getattr(self.processor, "tokenizer", None)
        if tokenizer is not None and getattr(tokenizer, "padding_side", None) != "left":
            tokenizer.padding_side = "left"

        # Build prompts using chat template format
        # Always include both user message and assistant response with ground truth
        # During evaluation, CustomTrainer's prediction_step will truncate before generation
        prompts = []
        for feature, label in zip(valid_features, label_texts):
            instruction = self._build_instruction(feature)

            conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "audio", "audio_url": None},
                        {"type": "text", "text": instruction}
                    ]
                },
                {
                    "role": "assistant",
                    "content": [
                        {"type": "text", "text": label}
                    ]
                }
            ]
            prompt = self.processor.apply_chat_template(
                conversation,
                add_generation_prompt=False,
                tokenize=False
            )
            prompts.append(prompt)

        # Process with the model processor
        inputs = self.processor(
            audio=audio_arrays,
            sampling_rate=self.sampling_rate,
            text=prompts,
            return_tensors="pt",
            padding=True,
        )

        # Create labels with proper masking
        labels = self._create_labels(inputs, label_texts, tokenizer)
        inputs["labels"] = labels

        return inputs

    def _filter_corrupted_audio(self, features: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Filter out corrupted audio samples.

        Override this method if you want to skip corrupted audio handling.
        """
        valid_features = []
        for feature in features:
            try:
                # Try to access the audio array to detect corruption early
                _ = feature["audio"]["array"]
                valid_features.append(feature)
            except (RuntimeError, Exception) as e:
                # Skip corrupted audio files
                print(f"Warning: Skipping corrupted audio sample: {e}")
                continue
        return valid_features

    def _create_labels(
        self,
        inputs: Dict[str, torch.Tensor],
        label_texts: List[str],
        tokenizer: Any,
    ) -> torch.Tensor:
        """Create label tensor with proper masking.

        Masks:
        - Padding tokens (pad_id)
        - Audio tokens (audio_token_id)
        - Everything before the assistant's response (instruction tokens)

        Only the assistant's actual response tokens are used for computing loss.
        """
        labels = inputs["input_ids"].clone()
        pad_id = self.processor.tokenizer.pad_token_id
        audio_token_id = self.processor.tokenizer.convert_tokens_to_ids(
            self.processor.audio_token
        )

        # Mask padding and audio tokens
        labels = labels.masked_fill(labels == pad_id, -100)
        labels = labels.masked_fill(labels == audio_token_id, -100)

        # Mask everything except the assistant's response
        for i, label in enumerate(label_texts):
            # Tokenize the ground truth label to identify it in the full sequence
            label_tokens = tokenizer.encode(label, add_special_tokens=False)

            # Find where the label appears in the input_ids
            input_ids = inputs["input_ids"][i]
            label_length = len(label_tokens)

            # Search for the label tokens in the sequence
            found = False
            for j in range(len(input_ids) - label_length + 1):
                if torch.all(input_ids[j:j + label_length] == torch.tensor(label_tokens, device=input_ids.device)):
                    # Mask everything before the label
                    labels[i, :j] = -100
                    found = True
                    break

            # Fallback: mask based on sequence structure
            if not found:
                non_masked = (labels[i] != -100).nonzero(as_tuple=False)
                if len(non_masked) > label_length:
                    mask_until = non_masked[-label_length].item()
                    labels[i, :mask_until] = -100

        return labels


@dataclass
class BaseClassificationCollator(BaseAudioTextCollator):
    """Base collator for classification tasks (intent, emotion, speaker ID).

    Subclasses only need to provide:
    - label_names: List of class names
    - A custom prompt template via _build_instruction
    """

    label_names: Sequence[str] = None

    def _label_to_text(self, value: Any) -> str:
        """Convert a label value (int or str) to its text representation."""
        if value is None:
            return ""
        try:
            index = int(value)
        except (TypeError, ValueError):
            return str(value)
        if 0 <= index < len(self.label_names):
            return str(self.label_names[index])
        return str(index)

    def _get_label_text(self, feature: Dict[str, Any]) -> str:
        """Extract label text from feature."""
        return self._label_to_text(feature.get("label"))


@dataclass
class BaseGenerationCollator(BaseAudioTextCollator):
    """Base collator for generation tasks (ASR, speech QA).

    For tasks where the output is free-form text rather than a class label.
    """

    def _get_label_text(self, feature: Dict[str, Any]) -> str:
        """Extract target text from feature.

        Override this method to specify which field contains the target text.
        """
        # Default: look for common text fields
        for field in ("text", "label_text", "answer", "transcript"):
            if field in feature and feature[field]:
                return str(feature[field])
        return ""


__all__ = [
    "BaseAudioTextCollator",
    "BaseClassificationCollator",
    "BaseGenerationCollator",
]
