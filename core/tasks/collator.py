"""Base data collators for audio-language tasks."""

from __future__ import annotations

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Sequence

import torch


def build_strict_label_mask(
    *,
    input_ids: torch.Tensor,
    labels: torch.Tensor,
    label_tokens: Sequence[int],
    ignore_index: int = -100,
) -> Dict[str, Any]:
    """Mask all tokens except the final assistant label span.

    The rightmost exact token match is used to avoid matching class labels
    that appear earlier in prompt instructions (e.g., "Choose from: ...").
    If no exact match is found, falls back to keeping the final non-masked
    `len(label_tokens)` token positions.
    """
    label_token_list = [int(tok) for tok in label_tokens]
    label_len = len(label_token_list)
    result: Dict[str, Any] = {
        "matched": False,
        "used_fallback": False,
        "fallback_reason": None,
        "label_token_count": label_len,
        "kept_token_count": 0,
    }

    if label_len <= 0:
        labels.fill_(ignore_index)
        result["used_fallback"] = True
        result["fallback_reason"] = "empty_label_tokens"
        return result

    # Find rightmost exact label span.
    rightmost_start: Optional[int] = None
    for start in range(int(input_ids.shape[0]) - label_len, -1, -1):
        if torch.equal(
            input_ids[start : start + label_len],
            torch.tensor(label_token_list, device=input_ids.device, dtype=input_ids.dtype),
        ):
            rightmost_start = int(start)
            break

    if rightmost_start is not None:
        labels[:rightmost_start] = ignore_index
        labels[rightmost_start + label_len :] = ignore_index
        valid = int((labels != ignore_index).sum().item())
        result["matched"] = True
        result["kept_token_count"] = valid
        return result

    # Deterministic fallback: keep the last non-masked tokens only.
    non_masked = (labels != ignore_index).nonzero(as_tuple=False).squeeze(-1)
    if non_masked.numel() <= 0:
        labels.fill_(ignore_index)
        result["used_fallback"] = True
        result["fallback_reason"] = "no_non_masked_tokens"
        return result

    keep_count = min(label_len, int(non_masked.numel()))
    keep_positions = non_masked[-keep_count:]
    labels.fill_(ignore_index)
    labels[keep_positions] = input_ids[keep_positions]
    result["used_fallback"] = True
    result["fallback_reason"] = "rightmost_match_not_found"
    result["kept_token_count"] = keep_count
    return result


@dataclass
class BaseAudioTextCollator(ABC):
    """Abstract base class for audio+text data collators.

    All collators using chat template format should inherit from this class.
    This implements the common pattern of:
    1. Building chat-formatted prompts with audio + text instruction
    2. Processing with the model's processor
    3. Creating labels with proper masking (padding, audio tokens, instruction tokens)

    Subclasses must implement:
    - _build_instruction: Create the instruction text for the user message
    - _get_label_text: Extract the target label/text from a feature
    """

    processor: Any
    sampling_rate: int
    include_transcript: bool = True
    warn_on_label_mask_fallback: bool = True

    def _build_instruction(self, feature: Dict[str, Any]) -> str:
        """Build the instruction text for the user message.

        Args:
            feature: A single example from the dataset

        Returns:
            The instruction text to be included in the user message
        """
        raise NotImplementedError("Subclasses must implement _build_instruction")

    def _get_label_text(self, feature: Dict[str, Any]) -> str:
        """Extract the label/target text from a feature.

        Args:
            feature: A single example from the dataset

        Returns:
            The target text that should be generated by the assistant
        """
        raise NotImplementedError("Subclasses must implement _get_label_text")

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """Prepare batch tensors for the trainer.

        This implements the standard pattern:
        1. Extract audio arrays and target labels
        2. Build chat-formatted prompts
        3. Process with the model processor
        4. Create labels with proper masking
        """
        # Filter out corrupted audio samples (optional - only for tasks that need it)
        valid_features = self._filter_corrupted_audio(features)
        if not valid_features:
            raise RuntimeError("All audio samples in batch are corrupted")

        # Extract audio and labels
        audio_arrays = [feature["audio"]["array"] for feature in valid_features]
        label_texts = [self._get_label_text(feature) for feature in valid_features]

        # Configure tokenizer padding
        tokenizer = getattr(self.processor, "tokenizer", None)
        if tokenizer is not None and getattr(tokenizer, "padding_side", None) != "left":
            tokenizer.padding_side = "left"

        # Build prompts using chat template format
        # Always include both user message and assistant response with ground truth
        # During evaluation, CustomTrainer's prediction_step will truncate before generation
        prompts = []
        for feature, label in zip(valid_features, label_texts):
            instruction = self._build_instruction(feature)

            conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "audio", "audio_url": None},
                        {"type": "text", "text": instruction}
                    ]
                },
                {
                    "role": "assistant",
                    "content": [
                        {"type": "text", "text": label}
                    ]
                }
            ]
            prompt = self.processor.apply_chat_template(
                conversation,
                add_generation_prompt=False,
                tokenize=False
            )
            prompts.append(prompt)

        # Process with the model processor
        inputs = self.processor(
            audio=audio_arrays,
            sampling_rate=self.sampling_rate,
            text=prompts,
            return_tensors="pt",
            padding=True,
        )

        # Create labels with proper masking
        labels = self._create_labels(inputs, label_texts, tokenizer)
        inputs["labels"] = labels

        return inputs

    def _filter_corrupted_audio(self, features: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Filter out corrupted audio samples.

        Override this method if you want to skip corrupted audio handling.
        """
        valid_features = []
        for feature in features:
            try:
                # Try to access the audio array to detect corruption early
                _ = feature["audio"]["array"]
                valid_features.append(feature)
            except (RuntimeError, Exception) as e:
                # Skip corrupted audio files
                print(f"Warning: Skipping corrupted audio sample: {e}")
                continue
        return valid_features

    def _create_labels(
        self,
        inputs: Dict[str, torch.Tensor],
        label_texts: List[str],
        tokenizer: Any,
    ) -> torch.Tensor:
        """Create label tensor with proper masking.

        Masks:
        - Padding tokens (pad_id)
        - Audio tokens (audio_token_id)
        - Everything before the assistant's response (instruction tokens)

        Only the assistant's actual response tokens are used for computing loss.
        """
        labels = inputs["input_ids"].clone()
        pad_id = self.processor.tokenizer.pad_token_id
        audio_token_id = self.processor.tokenizer.convert_tokens_to_ids(
            self.processor.audio_token
        )

        # Mask padding and audio tokens
        labels = labels.masked_fill(labels == pad_id, -100)
        labels = labels.masked_fill(labels == audio_token_id, -100)

        # Mask everything except the assistant's response
        for i, label in enumerate(label_texts):
            label_tokens = tokenizer.encode(label, add_special_tokens=False)
            input_ids = inputs["input_ids"][i]
            mask_stats = build_strict_label_mask(
                input_ids=input_ids,
                labels=labels[i],
                label_tokens=label_tokens,
                ignore_index=-100,
            )
            if self.warn_on_label_mask_fallback and bool(mask_stats.get("used_fallback", False)):
                reason = str(mask_stats.get("fallback_reason"))
                print(
                    "[collator] label mask fallback used "
                    f"(reason={reason}, label='{label}', kept={mask_stats.get('kept_token_count', 0)})"
                )

        return labels


@dataclass
class BaseClassificationCollator(BaseAudioTextCollator):
    """Base collator for classification tasks (intent, emotion, speaker ID).

    Subclasses only need to provide:
    - label_names: List of class names
    - A custom prompt template via _build_instruction
    """

    label_names: Sequence[str] = None

    def _label_to_text(self, value: Any) -> str:
        """Convert a label value (int or str) to its text representation."""
        if value is None:
            return ""
        try:
            index = int(value)
        except (TypeError, ValueError):
            return str(value)
        if 0 <= index < len(self.label_names):
            return str(self.label_names[index])
        return str(index)

    def _get_label_text(self, feature: Dict[str, Any]) -> str:
        """Extract label text from feature."""
        return self._label_to_text(feature.get("label"))


@dataclass
class BaseGenerationCollator(BaseAudioTextCollator):
    """Base collator for generation tasks (ASR, speech QA).

    For tasks where the output is free-form text rather than a class label.
    """

    def _get_label_text(self, feature: Dict[str, Any]) -> str:
        """Extract target text from feature.

        Override this method to specify which field contains the target text.
        """
        # Default: look for common text fields
        for field in ("text", "label_text", "answer", "transcript"):
            if field in feature and feature[field]:
                return str(feature[field])
        return ""


__all__ = [
    "build_strict_label_mask",
    "BaseAudioTextCollator",
    "BaseClassificationCollator",
    "BaseGenerationCollator",
]
