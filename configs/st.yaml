# Speech Translation (ST) Task Configuration - CoVoST2

task: st
seed: 0
language: en_ar                     # Language pair (affects prompts and artifact directories). Format: {source_lang}_{target_lang}

# Model Configuration
model:
  path: data/models/Qwen2.5-Omni-3B
  lora:
    r: 64                             # LoRA rank (higher = more parameters, better fit)
    alpha: 128                        # LoRA alpha scaling factor (typically 2x rank)
    dropout: 0.1                      # Dropout probability for LoRA layers
    bias: none                        # Bias type: none, all, or lora_only
    target_modules:                   # Modules to apply LoRA to
      - q_proj                        # Query projection for attention
      - k_proj                        # Key projection for acoustic feature matching
      - v_proj                        # Value projection for representation learning
      - o_proj                        # Output projection
      - audio_tower.proj              # Acoustic embeddings to LLM space (must train this)
      - gate_proj                     # MLP gating for complex acoustic patterns
      - up_proj                       # MLP expansion for representation
      - down_proj                     # MLP projection
    task_type: CAUSAL_LM

# Dataset Configuration
dataset:
  # Source
  dataset_name: fixie-ai/covost2      # CoVoST2 (language pair automatically set from top-level 'language' parameter)
  # Sample limits
  max_train_samples: 50000            # null = use all available
  max_validation_samples: 5000
  max_test_samples: 5000
  # Audio filtering
  max_duration: 20.0                  # Filter out audio longer than this (seconds)
  min_duration: 1.0                   # Filter out audio shorter than this (seconds)
  # Column mappings
  audio_column: audio                 # CoVoST2 specific column mappings
  source_column: sentence             # English source text
  translation_column: translation     # German translation
  # Splits
  train_split: train
  validation_split: validation
  test_split: test
  # Processing
  seed: 0
  num_proc: auto
  cache_splits: true
  force_rebuild: false

# Training Configuration
training:
  # Compute & batching
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2
  bf16: true
  fp16: false
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  # Optimization
  learning_rate: 3e-5
  lr_scheduler_type: cosine
  warmup_ratio: 0.03                  # Higher warmup for ST (complex task)
  weight_decay: 0.01
  max_grad_norm: 1.0
  # Schedule
  num_train_epochs: 3
  eval_strategy: steps
  eval_steps: 500
  save_strategy: steps
  save_steps: 500
  initial_eval: true
  # Checkpointing
  save_total_limit: 4
  resume_from_checkpoint: null        # Options: null (start fresh), "auto" (detect latest), or explicit path
  # Early stopping
  early_stopping_patience: 3
  early_stopping_threshold: 0       # BLEU improvement threshold
  load_best_model_at_end: true
  metric_for_best_model: bleu
  greater_is_better: true             # BLEU: higher is better
  # Data loading
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4
  group_by_length: false              # Important for variable length audio
  length_column_name: duration
  # Evaluation
  max_eval_samples: null              # Use subset during training for speed (null = full eval)
  shuffle_eval_subset: false
  eval_accumulation_steps: 2
  remove_unused_columns: false
  # Logging
  logging_steps: 10
  report_to:
    - tensorboard
    - wandb
  # Generation
  generation_kwargs:
    max_new_tokens: 128               # Translations can be longer
    do_sample: false
    num_beams: 4                      # Beam search for better translations
    length_penalty: 1.0               # Linear penalty (crucial for stable BLEU)

# Artifacts Configuration
artifacts:
  adapter_subdir: qwen2_5_omni_lora_st

# Metrics Configuration
metrics:
  history_csv: st_training_history.csv
  loss_plot: st_loss_bleu_plot.png
