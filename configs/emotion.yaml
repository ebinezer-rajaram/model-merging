# ============================================================================
# Emotion Recognition Task Configuration
# ============================================================================

task: emotion
seed: 0

# ============================================================================
# Model Configuration
# ============================================================================
model:
  path: models/Qwen2.5-Omni-3B

  # LoRA (Low-Rank Adaptation) Configuration
  lora:
    r: 64                    # LoRA rank (higher = more parameters, better fit)
    alpha: 128                # LoRA alpha scaling factor (typically 2x rank)
    dropout: 0.1             # Dropout probability for LoRA layers
    bias: none               # Bias type: none, all, or lora_only
    target_modules:          # Modules to apply LoRA to
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      # - gate_proj  # ADD: MLP gating
      # - up_proj    # ADD: MLP expansion
      # - down_proj  # ADD: MLP projection
      - audio_tower.proj       # L731: Critical for acoustic embeddings
    task_type: CAUSAL_LM

# ============================================================================
# Dataset Configuration
# ============================================================================
dataset:
  # Dataset source - using local MELD dataset
  dataset_name: local_meld
  dataset_config: default
  # Path to local MELD data directory
  data_dir: datasets/meld

  # Data processing
  seed: 0
  num_proc: auto
  cache_splits: true
  force_rebuild: false

  # Sample limits (null = use all)
  max_train_samples: null
  max_validation_samples: null
  max_test_samples: null

  # Maximum audio duration in seconds (null = no limit)
  # Samples exceeding this will be filtered out to prevent OOM
  # MELD average is ~3s, max normal sample is ~30s
  max_duration: 30.0

  # Minimum audio duration in seconds (null = no limit)
  # Filter out very short clips that lack context
  min_duration: 0.5

  # Audio normalization
  normalize_audio: false
  # Speaker-level normalization (normalize per speaker to handle different conditions)
  normalize_per_speaker: true
  # Target RMS level for normalization (typical range: -20 to -30 dB)
  target_rms_db: -25.0

  # Column mappings
  # MELD has: neutral, joy, sadness, anger, surprise, fear, disgust
  label_column: Emotion
  text_column: Utterance
  audio_column: audio

  # Split configuration
  # MELD has predefined train/validation/test splits
  train_split: train
  validation_split: validation
  test_split: test

  # Task-specific options
  include_transcript: false

# ============================================================================
# Loss Configuration
# ============================================================================
loss:
  # Loss function type: "default", "focal", "weighted"
  type: default

  # Focal loss parameters (only used if type: focal)
  focal:
    gamma: 2.0              # Focusing parameter (higher = more focus on hard examples)
    alpha: null             # Class weighting (null, single value, or list per class)

  # Weighted cross-entropy parameters (only used if type: weighted)
  weighted:
    # Option 1: Provide explicit weights for each emotion class
    # MELD emotions: neutral, joy, sadness, anger, surprise, fear, disgust
    weights: null           # e.g., [1.0, 1.5, 1.5, 1.5, 2.0, 2.5, 2.0]

    # Option 2: Auto-compute weights from training data distribution
    auto_compute: true      # If true, computes weights automatically
    method: sqrt_inverse    # Method for auto-compute: "inverse", "sqrt_inverse", "balanced"

# ============================================================================
# Training Configuration
# ============================================================================
training:
  # Batch sizes and accumulation
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 24
  gradient_accumulation_steps: 2

  # Optimization
  learning_rate: 2e-05
  lr_scheduler_type: cosine_with_restarts
  warmup_ratio: 0.1
  weight_decay: 0.05
  max_grad_norm: 1.0

  # Training duration
  num_train_epochs: 10

  # Checkpointing
  save_strategy: steps
  save_steps: 50
  save_total_limit: 3
  # Resume from checkpoint: null (start fresh), "auto" (detect latest), or explicit path
  resume_from_checkpoint: null

  # Evaluation
  eval_strategy: steps
  eval_steps: 50
  max_eval_samples: null        # Use subset during training for speed (full eval at end)
  shuffle_eval_subset: true     # Randomly sample eval subset
  initial_eval: true

  # Logging
  logging_steps: 10
  report_to:
    - tensorboard
    - wandb

  # Early stopping
  early_stopping_patience: 5
  early_stopping_threshold: 0.001
  load_best_model_at_end: true
  metric_for_best_model: eval_macro_f1
  greater_is_better: true

  # Performance optimizations
  bf16: true
  fp16: false
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false

  # Data loading
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 2

  # Balanced sampling for imbalanced datasets
  # Options: null (default), "balanced_batch", "weighted"
  # - "balanced_batch": Each batch has equal samples from each class
  # - "weighted": Oversample minority classes throughout training
  balanced_sampling: weighted

  # Weighting method for weighted sampling (only used if balanced_sampling: weighted)
  # Options: "inverse", "sqrt_inverse", "balanced"
  # - "inverse": Full rebalancing (minority classes sampled ~17x more)
  # - "sqrt_inverse": Gentler rebalancing (minority classes sampled ~4x more)
  # - "balanced": Scale to make expected samples equal per class
  sampling_method: sqrt_inverse

  group_by_length: false  # Disabled when using balanced_sampling
  length_column_name: duration

  # Other
  eval_accumulation_steps: 4
  remove_unused_columns: false

  # Generation parameters (for constrained decoding evaluation)
  generation_kwargs:
    max_new_tokens: 3        
    do_sample: false         # Greedy decoding for deterministic results
    num_beams: 1            # No beam search needed with constraints

# ============================================================================
# Artifacts Configuration
# ============================================================================
artifacts:
  adapter_subdir: qwen2_5_omni_lora_emotion_audio

# ============================================================================
# Metrics Configuration
# ============================================================================
metrics:
  history_csv: emotion_training_history.csv
  loss_plot: loss_accuracy_plot.png
  confusion_matrix: emotion_confusion_matrix.png
  normalize_confusion_matrix: true  # If true, shows percentages; if false, shows counts
