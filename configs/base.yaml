seed: 0
training:
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 1
  learning_rate: 5.0e-05
  lr_scheduler_type: cosine
  num_train_epochs: 2
  logging_steps: 25
  save_strategy: steps
  save_steps: 100
  save_total_limit: 3
  eval_strategy: steps
  eval_steps: 100
  early_stopping_patience: 2
  load_best_model_at_end: true
  metric_for_best_model: wer
  greater_is_better: false
  max_grad_norm: 1.0
  bf16: true
  report_to:
    - tensorboard
    - wandb
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  remove_unused_columns: false
  warmup_ratio: 0.05
  generation_kwargs:
    max_new_tokens: 128
    do_sample: false
