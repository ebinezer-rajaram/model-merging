# Speaker Verification Task Configuration

task: speaker_ver
seed: 0

# Model Configuration
model:
  path: data/models/Qwen2.5-Omni-3B
  lora:
    r: 64                             # LoRA rank (higher = more parameters, better fit)
    alpha: 128                        # LoRA alpha scaling factor (typically 2x rank)
    dropout: 0.1                      # Dropout probability for LoRA layers
    bias: none                        # Bias type: none, all, or lora_only
    target_modules:                   # Modules to apply LoRA to
      - q_proj                        # Query projection for attention
      - k_proj                        # Key projection
      - v_proj                        # Value projection
      - o_proj                        # Output projection
      - gate_proj                     # MLP gating
      - up_proj                       # MLP expansion
      - down_proj                     # MLP projection
      - audio_tower.proj              # Critical for acoustic embeddings
    task_type: CAUSAL_LM

# Dataset Configuration
dataset:
  # Source
  dataset_name: acul3/voxceleb2       # VoxCeleb2 dataset (124 speakers, 463k utterances)
  dataset_config: null
  # Processing
  seed: 0
  num_proc: auto
  cache_splits: true
  force_rebuild: false                 # Force rebuild to ensure we get fresh data
  # Sample limits
  max_speakers: 100                   # Select top 100 speakers by sample count
  pairs_per_speaker: 200              # Generate 200 pairs per speaker (100 pos + 100 neg)
  max_train_samples: null
  max_validation_samples: null
  max_test_samples: null
  # Audio filtering
  max_duration: 20.0                  # Filter out very long utterances (applied before pairing)
  min_duration: 1.0                   # Filter out very short utterances (applied before pairing)
  max_audio_length: 10.0              # Trim each audio to 10 seconds (before concatenation)
  audio_gap_seconds: 0.5              # Silence gap between audio_a and audio_b
  # Column mappings
  label_column: speaker_id            # VoxCeleb2 uses "speaker_id"
  text_column: transcription          # VoxCeleb2 uses "transcription"
  audio_column: audio
  # Splits
  split_by_speakers: true             # True = val/test speakers completely unseen (zero-shot); False = same speakers in all splits
  split_percentages:
    train: 0.8                        # 80% for training (80 speakers, ~16,000 pairs)
    validation: 0.1                   # 10% for validation (10 speakers, ~2,000 pairs)
    test: 0.1                         # 10% for test (10 speakers, ~2,000 pairs)
  train_split: train
  validation_split: validation
  test_split: test

# Loss Configuration
loss:
  type: default                       # Standard cross-entropy for binary classification. Options: "default", "focal", "weighted"
  focal:                              # Used only if type: focal
    gamma: 2.0                        # Focusing parameter (higher = more focus on hard examples)
    alpha: null                       # Class weighting (null or list per class)
  weighted:                           # Used only if type: weighted
    weights: null                     # Explicit weights for each class [weight_no, weight_yes]
    auto_compute: false               # Auto-compute weights from training data distribution
    method: sqrt_inverse              # Method for auto-compute: "inverse", "sqrt_inverse", "balanced"

# Training Configuration
training:
  # Compute & batching
  per_device_train_batch_size: 16   # Smaller batch size due to dual audio inputs (longer sequences)
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2
  bf16: true
  fp16: false
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  # Optimization
  learning_rate: 3e-05
  lr_scheduler_type: cosine
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  # Schedule
  num_train_epochs: 2
  eval_strategy: steps
  eval_steps: 50
  save_strategy: steps
  save_steps: 50
  initial_eval: true
  # Checkpointing
  save_total_limit: 3
  resume_from_checkpoint: null        # Options: null (start fresh), "auto" (detect latest), or explicit path
  # Early stopping
  early_stopping_patience: 10
  early_stopping_threshold: 0.001
  load_best_model_at_end: true
  metric_for_best_model: eval_macro_f1  # Binary classification: use accuracy
  greater_is_better: true
  # Data loading
  dataloader_num_workers: 16
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4
  group_by_length: false              # Don't group by length (pairs have variable combined length)
  balanced_sampling: null             # Pairs should already be balanced (50/50), so null is appropriate
  # Evaluation
  max_eval_samples: null              # Use subset during training for speed (null = full eval)
  shuffle_eval_subset: true           # Randomly sample eval subset
  eval_accumulation_steps: 4
  remove_unused_columns: false
  # Logging
  logging_steps: 10
  report_to:
    - tensorboard
    - wandb
  # Generation
  generation_kwargs:
    max_new_tokens: 2                 # "yes" or "no" are very short
    do_sample: false                  # Greedy decoding for deterministic results
    num_beams: 1                      # No beam search needed

# Artifacts Configuration
artifacts:
  adapter_subdir: qwen2_5_omni_lora_speaker_ver

# Metrics Configuration
metrics:
  history_csv: speaker_ver_training_history.csv
  loss_plot: speaker_ver_loss_accuracy.png
  confusion_matrix: speaker_ver_confusion_matrix.png
  normalize_confusion_matrix: true    # If true, shows percentages; if false, shows counts
